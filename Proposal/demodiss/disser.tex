\documentclass[12pt,twoside,notitlepage]{report}
\usepackage{a4}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[font={small,it}]{caption}
\usepackage{pdfpages}
\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Karina Palyutina}

\vspace*{60mm}
\begin{center}
\Huge
{\bf Machine learning inference of search engine heuristics} \\
\vspace*{5mm}
Part II Project \\
\vspace*{5mm}
St Catharine's College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Karina Palyutina                       \\
College:            & \bf St Catharine's College                     \\
Project Title:      & \bf Machine learning inference of search engine heuristics \\
Examination:        & \bf Part II Project        \\
Word Count:         & \bf \footnotemark[1]     \\
Project Originator: & Dr Jon Crowcroft                    \\
Supervisor:         & Dr Jon Crowcroft                  \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by {\tt detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}

\newpage


\section*{Original Aims of the Project}


\section*{Work Completed}


\section*{Special Difficulties}

\section*{Declaration of Originality}

I, Karina Palyutina of St Catharine's College, being a candidate for Part II of the Computer
Science Tripos , hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed }

\medskip
\leftline{Date }

\cleardoublepage

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering
                        % is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
This project is inspired by increasing importance of search engine rankings.
Today major search engines given a query return web pages in an order
determined by secret algorithms. Such algorithms are believed 
to incorporate multiple unknown factors.
For instance, Google claims to have over 200 unique factors that influence a
position of a webpage in the search results relative to a query
\footnote{http://www.google.com/competition/howgooglesearchworks.html}. Only
a handful of these factors are disclosed to the webmasters  in the form of very
general guidelines. Moreover, the Google algorithm in particular is updated
frequently. However, most of the knowledge around the area amounts to
speculation. Despite the fact that it is possible to pass a vast number of
queries through the black box of any existing search engine,the immensity of
the search space, and instability of such algorithms make them impossible to
reverse engineer.

Machine learning is a natural approach to inferring the true algorithm from a
subset of all possible observations. However, applying machine learning
techniques to real search engines would be hardly effective, as the dynamic
nature of the algorithms and the web as well as lack of meaningful feedback
would prevent incremental improvement: when there are as many as 200 features
in question, false assumptions made by a learner may have an unpredictable
effect on its performace.

More generally, there are certain ambiguities associated with machine learning,
which are 'problem-specific'. For example, it proves difficult to decide how
much training data is necessary, as well as and selecting it to avoid
over/under-fitting\cite{domingos}. Similarly, it is not straightforward which
machine learning technique is best for a particular problem.

This project is concerned with application of machine learning techniques to
search engines. The aim of the project, in particular,  is to explore how
machine learning techniques can be used effectively to infer algorithms from
search engines. To address the limitations imposed by existing search engines,
part of the task is to develop a toy search engine that allows me to comtrol
the nature and complexity of used heuristics. Such transparency addresses the
problems stated above and, more importantly,  allows for useful evaluation of
machine learning techniques by providing meaningful feedback.

Even though this study does not attempt to reverse engineer any existing
heuristics, the results can be applied to such an ambitious task.
Moreover, such a framework is potentially more general and can be used for a
range of problems.

\newcommand\todo[1]{\textcolor{red}{#1}}
\todo{overview of the chapters here.}

\cleardoublepage
\chapter{Preparation}

This chapter describes work that has been done before coding was started.  In
particular, it focuses on the reasoning behind the design of the system to be
implemented. The first section is devoted to research undertaken to determine
what can be done and how best to do it. The second section formulates the
system requirements, namely formalizes everything that is developed in this
project. The last section outlines the particulars of the software engineering
approach to be adopted by this project.

\section{Formulating the Goals}
A particular difficulty in this project has been in planning what has to be
done. Due to the exploratory nature of the project the course of action had to
be predominantly determined by the outcome of a current tactic. Moreover, the
unknowns originating from the machine learning further complicated matters. 

\paragraph{System Overview.}

To achieve the goal of the project, a machine learning techniques comparison
framework was necessary. In the Introduction I mentioned the benefit of having
a transparent system as an object of learning. To further justify this
decision, it is worth mentioning that  generalisation using machine learning is
different from most optimization problems in that the function that is being
optimized is out of our reach, and all that is visible to the machine learner
is the training error. Because our goal is not the correct classification of
real data, but identifying the means to correct classification, it is important
that informed choices are made towards improvement of the learner. Taking this
into account, knowing the function that we want to learn and having direct
control over it  will guide the improvement of the search engine. 

This argument motivates a system in three parts: a search engine, a machine
learner and a parser to mediate between the two.
Figure \ref{overview} illustrates the proposed learning system. Training data
is a set of web pages set aside specifically for training purposes. Such web
pages are not required to have any special properties, but diversity and
typicality are seen as advantageous. As for the size of the training data, 
Domingos \cite{domingos} suggests that a primitive learner given more data
performs better than a more complex one with less data. This, of course, is
under certain assumptions of data quality, namely the assumption that the
training data is a representative subset of all the possible data. Intuitively,
provided there is no bias in data gathering, more data implies better
generality. I have started with a training set spanning an order of a few
thousands of pages, however, in practice, I found that there is no
particular improvement beyond a thousand pages. \todo{Link to relevant part or
example data here?}

\paragraph{Data.}

\paragraph{Search Engine.}

Next important decision regarded the search engine.  Originally, I considered
using open source existing engines, in particular, Lucene. Even though I could
freely modify it for the purposes of the project, the complexity of it was
superfulous. I saw writing a simple search engine as a more beneficial
exercise, as developing it in the first place potentially gives an insight into
the problem.

Functionally our search engine is a black box that takes a set of webpages and
a set of queries and outputs an order. Or a score!

\paragraph{Language.}

When choosing a programming language, main considerations reduced to library
availibility and simplicity. The project imposes no special  requirements on the
language, apart from, perhaps, library infrastructure for parsing web pages.
Python is simple language with extensive library support. As
for efficiency, all the mathematical operations in this project rely on python
math libraris, which are implemented in C. I have not programmed in Python
before the project, so a slight overhead was caused by having to learn a new
language.

\paragraph{Machine Learning.}

I have now covered main peripheral decisions, but it is machine learning that
constitutes the central part of the project. The field was completely new to me
to start with, so research of different techniques was a big part of the
preparation.

It is generally recommended that the simplest learners are tried
first\cite{domingos}. Of all learners Naive Bayesian is one of the most
comprehensible.This in itself is a major advantage according to the Occam's
razor principle, which finds ample application in machine learning.

Naive Bayes is a probabilistic classifier based on the Bayes Theorem. The
posterior probability \(P(C|\vec{F})\) denotes the probability that a sample
page with a feature vector \(\vec{F}=(F_1,F_2,\dots,F_n)\) belongs to class C.
The posteriior probability is computed from the observable in the training data: the prior
probability \(P(C)\) -- the unconditional probability of a page belonging to
the class C, the likelihood \(P(\vec{F}|C)\) and the evidence \(P(\vec{F})\):
\begin{equation}
P(C|\vec{F}) = \frac{P(C)P(\vec{F}|C)}{P(\vec{F})}
\end{equation}

The simplicity of Bayesian approach owes to the conditional independence
assumption: each \(F_i\) in \(\vec{F}\) is assumed to be independent of one
another to get \(P(\vec{F}|C)=P(F_1|C)*P(F_2|C)*\dots*P(F_n|C)\). This leads to a concise classifier definition:
\begin{equation}
\hat{C}= argmax_C P(C)\prod_{i=1}^{n}P(F_i|C)
\end{equation}
where \(C\) is the result of classification of a page with feature vector
\(F_1,F_2,\dots,F_n\).

In practice, the crude assumption rarely  holds and is likely to
be violated by our data, as we expect features of pages to be interdependent.
However, it has been shown that Naive Bayes performs well under zero-one loss
function in presence of dependencies\cite{domingos96}. This has a few
implications for this project, particularly, on evaluation methods. 


\paragraph{Evaluation methodology.}
Baseline:Bayes and ceiling
\section{Requirements Analysis}
Search engine: efficiency!
Machine learner: simplicity (domingos) occam's razor
Interface between: hiding, encapsulating
Evaluability
\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/overview.pdf}
\caption{Overview of the system. Three major parts from left to right are search engine,
parser and machine learner.}
\label{overview}
\end{figure}

Figure \ref{eval} shows the evaluation strategy. The Test Data is the data
carefully set aside at the beginning that is never exposed to the learner. 

Ranking vs scoring 
\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/eval.pdf}
\caption{Evaluation system. }
\label{eval}
\end{figure}
\section{Development Strategy}

\cleardoublepage
\chapter{Implementation}
This section describes parts of the system that I have implemented.

\section{Search Engine}
Overview

\subsection{PageRank}
\subsection{Indexer}
\subsection{Optimization}

\section{Parser}

\section{Machine Learning}
Overview
\subsection{Naive Bayes}

\subsection{Support Vector Machine}

\cleardoublepage
\chapter{Evaluation}

\cleardoublepage
\chapter{Conclusion}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

%\input{propbody}

\end{document}
