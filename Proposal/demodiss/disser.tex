\documentclass[12pt,twoside,notitlepage]{report}
\usepackage{a4}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[font={small,it}]{caption}
\usepackage{pdfpages}
\usepackage{amsmath}
\usepackage{bm}
\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Karina Palyutina}

\vspace*{60mm}
\begin{center}
\Huge
{\bf Machine learning inference of search engine heuristics} \\
\vspace*{5mm}
Part II Project \\
\vspace*{5mm}
St Catharine's College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Karina Palyutina                       \\
College:            & \bf St Catharine's College                     \\
Project Title:      & \bf Machine learning inference of search engine heuristics \\
Examination:        & \bf Part II Project        \\
Word Count:         & \bf \footnotemark[1]     \\
Project Originator: & Dr Jon Crowcroft                    \\
Supervisor:         & Dr Jon Crowcroft                  \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by {\tt detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}

\newpage


\section*{Original Aims of the Project}


\section*{Work Completed}


\section*{Special Difficulties}

\section*{Declaration of Originality}

I, Karina Palyutina of St Catharine's College, being a candidate for Part II of the Computer
Science Tripos , hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed }

\medskip
\leftline{Date }

\cleardoublepage

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering
                        % is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
This project is inspired by increasing importance of search engine rankings.
Today major search engines given a query return web pages in an order
determined by secret algorithms. Such algorithms are believed 
to incorporate multiple unknown factors.
For instance, Google claims to have over 200 unique factors that influence a
position of a webpage in the search results relative to a query
\footnote{http://www.google.com/competition/howgooglesearchworks.html}. Only
a handful of these factors are disclosed to the webmasters  in the form of very
general guidelines. Moreover, the Google algorithm in particular is updated
frequently. However, most of the knowledge around the area amounts to
speculation. Despite the fact that it is possible to pass a vast number of
queries through the black box of any existing search engine,the immensity of
the search space, and instability of such algorithms make them impossible to
reverse engineer.

Machine learning is a natural approach to inferring the true algorithm from a
subset of all possible observations. However, applying machine learning
techniques to real search engines would be hardly effective, as the dynamic
nature of the algorithms and the web as well as lack of meaningful feedback
would prevent incremental improvement: when there are as many as 200 features
in question, false assumptions made by a learner may have an unpredictable
effect on its performace.

More generally, there are certain ambiguities associated with machine learning,
which are 'problem-specific'. For example, it proves difficult to decide how
much training data is necessary, as well as and selecting it to avoid
over/under-fitting\cite{domingos}. Similarly, it is not straightforward which
machine learning technique is best for a particular problem.

This project is concerned with application of machine learning techniques to
search engines. The aim of the project, in particular,  is to explore how
machine learning techniques can be used effectively to infer algorithms from
search engines. To address the limitations imposed by existing search engines,
part of the task is to develop a toy search engine that allows me to comtrol
the nature and complexity of used heuristics. Such transparency addresses the
problems stated above and, more importantly,  allows for useful evaluation of
machine learning techniques by providing meaningful feedback.

Even though this study does not attempt to reverse engineer any existing
heuristics, the results can be applied to such an ambitious task.
Moreover, such a framework is potentially more general and can be used for a
range of problems.

\newcommand\todo[1]{\textcolor{red}{#1}}
\todo{overview of the chapters here.}

\cleardoublepage
\chapter{Preparation}

This chapter describes work that has been done before coding was started.  In
particular, it focuses on the reasoning behind the design of the system to be
implemented. The first section is devoted to research undertaken to determine
what can be done and how best to do it. The second section formulates the
system requirements, namely formalizes everything that is developed in this
project. The last section outlines the particulars of the software engineering
approach to be adopted by this project.

\section{Formulating the Goals}
A particular difficulty in this project has been in planning what has to be
done. Due to the exploratory nature of the project the course of action had to
be predominantly determined by the outcome of a current tactic. Moreover, the
unknowns originating from the machine learning further complicated matters. 

\paragraph{System Overview.}

To achieve the goal of the project, a machine learning techniques comparison
framework was necessary. In the Introduction I mentioned the benefit of having
a transparent system as an object of learning. To further justify this
decision, it is worth mentioning that  generalisation using machine learning is
different from most optimization problems in that the function that is being
optimized is out of our reach, and all that is visible to the machine learner
is the training error. Because our goal is not the correct classification of
real data, but identifying the means to correct classification, it is important
that informed choices are made towards improvement of the learner. Taking this
into account, knowing the function that we want to learn and having direct
control over it  will guide the improvement of the search engine. 

This argument motivates a system in three parts: a search engine, a machine
learner and a parser to mediate between the two.  Figure \ref{overview}
illustrates the proposed learning system. Training data is a set of web pages
set aside specifically for training purposes.  

\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/overview.pdf}
\caption{Overview of the system. Three major parts from left to right are search engine,
parser and machine learner.}
\label{overview}
\end{figure}

\paragraph{Data.}

Web pages used as Training and Test data are not required to have any special
properties, but diversity and typicality are seen as advantageous. As for the
size of the training data, Domingos \cite{domingos} suggests that a primitive
learner given more data performs better than a more complex one with less data.
This, of course, is under certain assumptions of data quality, namely the
assumption that the training data is a representative subset of all the
possible data. Intuitively, provided there is no bias in data gathering, more
data implies better generality. I have started with a training set spanning an
order of a few thousands of pages, however, in practice, I found that there is
no particular improvement beyond a thousand pages. \todo{Link to relevant part
or example data here?}

\paragraph{Search Engine.}

Next important decision regarded the search engine.  Originally, I considered
using open source existing engines, in particular, Lucene. Even though I could
freely modify it for the purposes of the project, the complexity of it was
superfulous. I saw writing a simple search engine as a more beneficial
exercise, as developing it in the first place potentially gives an insight into
the problem.

Functionally our search engine is a black box that takes a set of webpages and
a set of queries and outputs an order. The order is determined by the features
of the page, which together make up a score. The score is the
function we want to infer using the ranking assigned by the search engine,
however, we are only given the order as evidence. Machine learning paragraph
below will address this issue in more details.

In general there are two aspects of information retrieval that have to be
accounted for: precision and recall. Precision is the fraction of retrieved pages that
are relevant to the query, whereas recall is the fraction of relevant
documents that are retrieved. Even though both are important for a good search
engine, but in practice, the web is very large, and so precision, or even
precision at n\footnote{Precision at n only evaluates precision with respect to
n topmost returned pages.}) has become
more prominent in defining a good search engine: very rarely the user actually
browses returns that are not in the top few tens of returned pages. Therefore, 
modern search engines tend to focus on high precision at the expense of recall
\cite{GOOGLE}. Therefore, we will concentrate primarily on precision, when
designing a search engine.


\paragraph{Pagerank} 
The PageRank algorithm was implemented as described in the original paper
by Page and Brin\cite{pagerank}.
\paragraph{Language.}

When choosing a programming language, main considerations reduced to library
availibility and simplicity. The project imposes no special  requirements on the
language, apart from, perhaps, library infrastructure for parsing web pages.
Python is simple language with extensive library support. As
for efficiency, all the mathematical operations in this project rely on python
math libraris, which are implemented in C. I have not programmed in Python
before the project, so a slight overhead was caused by having to learn a new
language.

\paragraph{Machine Learning.}

I have now covered main peripheral decisions, but it is machine learning that
constitutes the central part of the project. The field was completely new to me
to start with, so research of different techniques was a big part of the
preparation.

It is generally recommended that the simplest learners are tried
first\cite{domingos}. Of all learners Naive Bayesian is one of the most
comprehensible.This in itself is a major advantage according to the Occam's
razor principle, which finds ample application in machine learning.

\paragraph{Naive Bayes.}Naive Bayes is a probabilistic classifier based on the Bayes Theorem. The
posterior probability \(P(C|\vec{F})\) denotes the probability that a sample
page with a feature vector \(\vec{F}=(F_1,F_2,\dots,F_n)\) belongs to class C.
The posteriior probability is computed from the observable in the training data: the prior
probability \(P(C)\) -- the unconditional probability of a page belonging to
the class C, the likelihood \(P(\vec{F}|C)\) and the evidence \(P(\vec{F})\):
\begin{equation}
P(C|\vec{F}) = \frac{P(C)P(\vec{F}|C)}{P(\vec{F})}
\end{equation}

The simplicity of Bayesian approach owes to the conditional independence
assumption: each \(F_i\) in \(\vec{F}\) is assumed to be independent of one
another to get \(P(\vec{F}|C)=P(F_1|C)*P(F_2|C)*\dots*P(F_n|C)\). This leads to a concise classifier definition:
\begin{equation}
\hat{C}= argmax_C P(C)\prod_{i=1}^{n}P(F_i|C)
\end{equation}
where \(C\) is the result of classification of a page with feature vector
\(F_1,F_2,\dots,F_n\).

In practice, the crude assumption rarely  holds and is likely to
be violated by our data, as we expect features of pages to be interdependent.
However, it has been shown that Naive Bayes performs well under zero-one loss
function in presence of dependencies\cite{OPTIM}. This has a few
implications for this project, particularly, on evaluation methods 

As we have seen, Naive Bayes assigns probabilities to possible classifications
in the process of classifying. Even though it generally performs well in
classification tasks, these probability estimates are poor \cite{domingos96}.
However, despite poor probability estimates, there exist several frameworks,
which make use of Bayesian classification and achieve decent performance in
ranking. For example, Zhang \cite{zhang04} experimentally found that Naive
Bayes is locally optimal in ranking. The paper defines a classifier as locally
optimal in ranking a positive example E if there is no negative example ranked
after E and vice versa for a negative exaple. A classifier is global in ranking
if it is locally optimal for all examples in the example set: in other words,
it is optimal in pairwise ranking.  It is particularly interesting that the
paper discovered that Naive Bayes is globally optimal in ranking on linear
functions that have been shown as not learnable by Naive Bayes\footnote{m-of-n
concepts and conjunctive concepts can't be learnt by Naive Bayes classifier but
can be optimally ranked by it according to Zhang \cite{zhang04}.}.
Another framework for ranking \cite{bayesrank} is based on Placket-Luce model, which reconciles
the concepts of score and rank. This framework is based on minimizing the Bayes
risk over possible permutations.

Existence of such frameworks suggest that Naive Bayes is an adequate choice
for this project. Classification is frequently opposed to regression, so
another approach covered by this project is Support Vector Regression. In
particular, \(\epsilon\)-Support Vector Regression.

\paragraph{\(\epsilon\)-Support Vector Regression.}
While the binary classification problem has as its goal the maximization of the
margin between the classes, regression is concerned with fitting a hyperplane
through the given training sequence.  A training sequence is a set of training
points \(D = \{ (\mathbf{x_1},t_1), (\mathbf{x_2},t_2), ... ,
(\mathbf{x_l},t_l) \}\) where \( \mathbf{x_i} \in R^n \) is a feature vector
holding features of pages and \( \mathbf{t_i} \in R \) is the corresponding
ranking of each page.

\begin{figure}
  \centering 
  \includegraphics[width=120mm]{epsilon.jpg}
  \caption{TODO: plot these yourself!} 
  \label{eps} 
\end{figure}

In simple linear regression the aim is to minimize a regularized error
function. We will be using an \(\epsilon\)-insensitive error function(see
Figure \ref{eps} (a)).

\(E_\phi(y(\mathbf(x)-t)) = \left\{ \begin{array}{l l} 0 & \quad \text{if
\(|y(\mathbf{x})-t)|<\epsilon\)}\\ |y(\mathbf{x})-t)|-\epsilon & \quad
\text{otherwise} \end{array} \right.\)

where \(y(\mathbf(x) = \mathbf{w^T}\phi(\mathbf{x})+b\) is the hyperplane
equation (and so \(y(\mathbf{x}) \) is the predicted output) and \(t_n\) is the
target (true) output.

The regression tube then contains all the points for which \(
y(\mathbf{x_n})-\epsilon \leq t_n \leq y(\mathbf{x_n})+\epsilon \) as shown in
Figure \ref{eps}(b).

To allow variables to lie outside of the tube, slack variables \(\xi_n \geq
0\)and \(\xi_n^* \geq 0\) are introduced.  The standard formulation of the
error function for support vector regression (ref Vapnik 1998) can be written
as follows:

\begin{gather} E= C\sum_{n=1}^{N}(\xi_n+\xi_n^*)+\frac{1}{2}\|\mathbf{w}\|^2
\end{gather}

\(E\) must be minimized subject to four constraints:

\begin{gather} \xi_n\geq 0,\\ \xi_n^*\geq 0,\\ t_n \leq
  y(\mathbf{x_n})+\epsilon+\xi_n,\\ t_n \geq y(\mathbf{x_n})-\epsilon-\xi_n^*,
\end{gather}

This constraint problem can be transformed into its dual form  by introducing
Lagrange multipliers \(a_n \geq 0, a_n^* \geq 0\).  The dual problem involves
maximizing

\begin{gather} L(\mathbf{a},\mathbf{a^*}) =
  -\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(a_n-a_n^*)(a_m-a_m^*)K(\mathbf{x_n},\mathbf{x_m})
\end{gather} \begin{gather*} -\epsilon\sum_{n=1}^{N}(a_n+a_n^*) +
  \sum_{n=1}^{N}t_n(a_n-a_n^*) \end{gather*}

where \(K(x_n,x_m) \) is the kernel function, \(t_n\) is the target output,

subject to constraints

\begin{gather}
  \sum_{n=1}^{N}(a_n-a_n^*)=0,\\
  0\leq a_n,\; a_n^*\leq C,\;\;    n=1,...,l 
\end{gather}


\paragraph{Evaluation methodology.}
Two proposed techniques Naive Bayes and SVM regression are quite different, so
comparing them is potentially erroneous. However, comparisons can be done
within each method, as there is a lot of scope for variety of implementations
in each.
As a baseline for the Bayesian approach, a very primitive ranking model will be
used. We will simply disregard the scoring function behind the rank and
directly infer the ranking funciton instead. More sophisticated ranking models
can then be compared to this basic performance.

\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/eval.pdf}
\caption{Evaluation system. }
\label{eval}
\end{figure}

Figure \ref{eval} shows an evaluation framework for Naive Bayes.  The Test Data
is the data carefully set aside at the beginning that is never exposed to the
learner. 

For an SVM learner the baseline can be set to the performance using a linear
kernel below.

\begin{gather}
K(x,y) = x . y
\end{gather}
where `.' is the dot product.

This is expected to be very high for linear heuristics but a lot lower for
non-linear ones.

\section{Requirements Analysis}
The resulting system must comply with the requirements stated below. 
\begin{itemize}
  \item{A search engine, which given a query must return a relevant subset of
      pages within 10 seconds in an order corresponding to a given heuristic, which can also be
  supplied to the search engine.}
  \item{Search engine must base ranking decisions on both dynamic
      (query dependent) and static (query independent)
    page features.}
  \item{A machine learner must be sufficiently isolated from the search engine
    through an encapsulating interface to avoid the possibility of undesirable interaction.}
  \item{Machine learners must be able to process thousands of pages in a
    reasonable time.}
  \item{The evaluation module must write results to persistent storage.}
\end{itemize}
\section{Development Strategy}
While the set up of Part II projects encourages waterfall-like development
model, this project takes an iterative aproach. The first iteration renders a
prototype: a primitive search engine with a Naive Bayesian baseline classifier.
The next iteration modifies each part of the system towards a more complex
solution. Evaluation is performed at each iteration. Within each  iteration the
development follows the evolved waterfall model -- the inremental build model.
Each increment represents a functionally separate unit of the system: a search
engine, a learner, a parser and an assessment module. Increments are developed
sequentially and regression testing is performed separately before integration.

The backup of the code is twofold: every time a substantial change is made a remote
version control repository is updated to hold the newest version. Regularly both the code and
the data used and obtained during evaluation are also backed up onto an
external hard drive. 

During the development of the learners a developent pool of pages will
be used, which must be disjoint with the Training or Test data. This ensures
that no optimization is tailored to the data used for evaluation. 
\cleardoublepage
\chapter{Implementation}
This section describes parts of the system that I have implemented.
\todo{overview}

\section{Data}
Development, Test and Training data for this project was implemented
Separate disrectories..

\section{Search Engine}

The first basic block of the system -- the search engine -- can be logically
split into two main parts: an indexer and a sorter. Because we concentrate on
precision, as discussed in the Preparation chapter, we will assume for
simplicity that all relevant documents are returned. This allowed me to an use
existing library implementation of the indexer and focus on the sorter.

\paragraph{Indexer}

The requirements on the indexer included flexibility, speed of indexing and
retrieval, simplicity and usability.  The `Whoosh' python library provides all
of these, so I used it to build an indexer. `Whoosh' is an open source indexing
library, so I had the option of modifying any part of it. It is  built in pure
Python, so it has a clean pythonic API. Its primary advantage is fast indexing
and retireval, although we are mostly concerned with retrieval speed, as
indexing is done rarely. The predecessors of `Whoosh' have served as the basis
of well-known browsers such as Lucene, so it is also a powerful indexing tool,
should I have needed more sophistication.

I have defined a very simple schema for indexing. Perhaps, one notable detail
is that `Whoosh' can store timestamps with the index, which enabled me to
provide both clean and incremental index methods. The incremental indexing
relies on the timestamp stored with the index and compares it to the
last-modified time provided by the file system. The user can specify whether
indexing has to be done from scratch or updated to acommodate some document
changes or document addition/deletion. I haven't originally expected to need an
incremental indexing capability, but throughout the projecct it has permitted
for a significant speedup.

\paragraph{Sorter}
Previously, I have defined the sorter as a logical unit that provides ranking to the
retrieved documents. For the first prototype, the sorter returned the pages in
the order of decreasing pagerank.
Subsequently, the sorting has been decoupled from retrieval and is described in
more detail in the Parser section.

\paragraph{PageRank} is computed using matrix inversion. All matrix operations
were performed with the help of the python numerical library `numpy'.  Take t
to be the teleportation probability, s=1-t is the probability of following a
random link, E is the teleportation probability: equiprobable transitions, as
using non-personalized pagerank (see Preparation), \(E_i,j = 1/N\) for all i,
j. G is a stochastic matrix holding the link structure of the data, such that 
\begin{equation*}
  G_i,j = \begin{cases}
    1/L & \text{there is a link from i to j and L = number of links from i}\\
    0   & \text{there is no link from i to j}
  \end{cases}
\end{equation*}
Then M is a stochastic matrix representing the web surfer activity, such
that \(M_i,j\) is the probability of going from page i to page j, 
\begin{equation} \label{1}
  M = s*G +t*E
\end{equation}
In one step the new location of the surfer is described by the distribution Mp.
We want to find a stationary distribution p, so must have
\begin{equation}\label{2}
  p = M*p
\end{equation}
Substituting \ref{1} into \ref{2}
\begin{equation} \label{a}
  p = (s*G+t*E)*p = s*G*p + t*E*p
\end{equation}
Rearranging equation \ref{a} gives
\begin{equation}
  p*(I-s*G) = t*E*p
\end{equation}
where I is the identity matrix

We can express E*p as P where P is a vector
\([\overbrace{1/N,1/N,\dots,1/N}^N].T\),as members of p must sum to one.
So computing pagerank amounts to
\begin{equation}
p  = t*(I-s*G)^{-1}*P
\end{equation}
where \((I-s*G)^{-1}\) denotes a matrix inverse operation.

This solution is simple at the expense of being slow. Although computing
inverse of a matrix is computationally expensive, we don't need to scale beyond
a few thousands of pages.  To avoid recomputation, I used python object
serialization module - Pickle to store the pagerank vector for each directory.
The resultant performance was actually very reasonable, the time spent
computing pagerank was insignificantly small in comparison to the time spent
crawling the directory.

\paragraph{Crawler}
The matrix G, used for the pagerank computation, represents random link
following activity. To obtain such a link structure each page has to be parsed,
and all links recorded. Because our data is obtained from a single source page
by a Wget spider, every page in a directory is guaranteed to be discovered by a
spider.

The Crawler class recursively traverses the pages depth first starting with the seed page,
the same as the seed page used for recursively downloading the pages from the
web. To make sure each page is only explored once, a dictionary is used to
hold pairs of absolute path, which uniquely identifies the page, and a
numerical value corresponding to the timestamp when the page has been first
discovered.

Although every page has a unique path, the links to other pages are relative.
Such links need to be normalized to maintain consistency.
A page object is used to encapsulate path complexity: all link paths are
converted to absolute paths before addition to the dictionary.
All outbound links are stored with the page in a Set datastrcture,
such that no link is added more than once. 

To produce the stochastic matrix G, we start with an empty NxN matrix, where N
is the total number of pages. We assume that whenever a surfer encounters a dangling page -- a page that has no
outbound links -- a teleportation step occurs. Therefore, every dangling page
links to every page in the pool including itself with equal probability 1/N. For non-dangling
pages, all links are assumed equiprobable and all pages that are not linked to
have probability of 0. So if page A links to pages B and C, but not itself or
D, its row in G is described by the Table \ref{tab}.

\begin{table}
    \begin{center}
      \begin{tabular}{|l|r|}
        \hline
        Page & A \\ \hline
         A &  0  \\ \hline
         B & 1/2 \\ \hline
         C & 1/2 \\ \hline
         D & 0   \\ \hline
      \end{tabular}
      \caption{Illustration of non-dangling pages: B and C share A's `importance' equally.\label{tab}}
  \end{center}
\end{table}

\section{Parser}

So far we have looked at the search engine. As a functional unit the search
engine only retrieves the relevant pages. The Parser is an abstraction for a
few classes which together perform a series of tasks involving page parsing. 
Search engine heuristics are also handled by the parsing module.

The primary function of the parser is to compute feature vectors for pages.
At this point we abstract away from the web pages and html parsing:
as far as machine learner is concerned, a feature vector is a complete
representation of a page.
The infrastructure for feature vector computation is easily extensible to
accommodate other related tasks, namely, training set and test set generation.
To fulfil the specification, I have taken an object oriented approach to the
design of the module. Figure \ref{uml} summarizes the design of the module in a
UML class diagram.

\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/uml.png}
\caption{A UML class diagram describing operation of the Parser.}
\label{uml}
\end{figure}

\section{Optimization}
\section{Machine Learning}
Overview
\subsection{Naive Bayes}

\subsection{Support Vector Machine}

The maximization problem (1.8) can be expressed as a minimization problem

\begin{gather}
  min_{\bm{\alpha},\bm{\alpha^*}} \frac{1}{2}(\bm{\alpha-\alpha^*})^T P (\bm{\alpha - \alpha^*})+\epsilon 
\sum_{i=1}^{l}(\alpha_i+\alpha_i^*)-\sum_{i=1}^{l}t_i(\alpha_i-\alpha_i^*)
\end{gather}
subject to

\begin{gather}
  \mathbf{e(\bm{\alpha}}-\bm{\alpha^*})=0 \\
  0\leq \alpha_i,\alpha_i^* \leq C, \;i=1,...,l
 \end{gather}

where
\(\mathbf{e}=[1,...,1],\;P_{ij}=K(x_i,x_j),\;t_i\) is the target output, \(C > 0\) and \(\epsilon > 0.\)

Solving a constrained optimization problem requires a quadratic programming solver. Cvxopt is one of the few python
libraries that implements a QP solver. The specification to the QP function is given as follows:

\(cvxopt.solvers.qp(P,q,G,h,A,b)\) solves a pair of primal and dual convex quadraic programs
\begin{gather}
  \min \frac{1}{2} x^T P x + q^T x
\end{gather}

 subject to

\begin{gather}
 G x \leq h\\
  Ax = b
\end{gather}

The goal was, therefore, to express (9) in terms of (12) and their respective constraints.
Described below is the transformation I devised to solve equation (9) using existing implementation that requires (12).
We take x to encode both \(\bm{\alpha}\) and \(\bm{\alpha^*}\) simultaneously, treating the upper half of x as 
\(\bm{\alpha}\) and the lower half 
as \(\bm{\alpha^*}\):


\[x =
  \begin{bmatrix}
    \bm{ \alpha} \\
    \bm{ \alpha^*}
  \end{bmatrix}
\]

Take P in equation (12) as 

\[P =
\begin{bmatrix}
       K & -K           \\
       -K & K            
     \end{bmatrix}
\]

where \(K_{ij} = K(x_i,x_j)\).

Observe that now \(x^T P x\) is the same as 
\(\sum_{n=1}^{N}\sum_{m=1}^{N}(a_n-a_n^*)(a_m-a_m^*)K(x_n,x_m)\) in (6) and 

Take q to hold the rest of (9).
\[q =
\begin{bmatrix}
  \epsilon*\mathbf{e}-t_0           \\
  \vdots                                     \\
  \epsilon*\mathbf{e}-t_{N-1}           \\
   \epsilon*\mathbf{e}+t_0           \\
   \vdots                           \\
  \epsilon*\mathbf{e}+t_{N-1}           \\
     \end{bmatrix}
\]

Now to encode constraint (11) consider the following pair of G and h

\[G =
\begin{matrix} %This is the super matrix
    \begin{matrix}   %One-row matrix to hold the brace
      \overbrace{\hphantom{\begin{matrix}-1 & \cdots & -1\end{matrix}}}^{\text{\footnotesize N}}
                                  &
      \overbrace{
        \hphantom{\begin{matrix}-1 & \cdots & -1\end{matrix}}
      }^{\text{\footnotesize N}}
    \end{matrix}
    &
  \\
\begin{bmatrix}
 -1 & 0 & 0 & 0 & 0 & 0\\
  0 & -1 & 0 & \cdots & \cdots & 0\\
  0 & 0 & \ddots  & 0 & \cdots & 0\\
  0 & \cdots & 0 & \ddots & 0 & 0\\
  0 & \cdots & \cdots & 0 & -1 & 0\\
  0 & 0 & 0 & 0 & 0 & -1 \\
  1 & 0 & 0 & 0 & 0 & 0\\
  0 & 1 & 0 & \cdots & \cdots & 0\\
  0 & 0 & \ddots  & 0 & \cdots & 0\\
  0 & \cdots & 0 & \ddots & 0 & 0\\
  0 & \cdots & \cdots & 0 & 1 & 0\\
  0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
  &
    %(2,2) cell: Actual matrix
    \begin{matrix}    %One-column matrix to hold a brace
      \vphantom{0} \\ %Blank space to skip first row
        \left.\vphantom{\begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\end{matrix}}\right\}
      \text{\footnotesize 2N} \\
        \left.\vphantom{\begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}}\right\}
      \text{\footnotesize 2N} 
    \end{matrix}
    %The inter-column spacing of the super matrix looks too big by default
    \mspace{-33mu}
\end{matrix}
\]


\[h = [\overbrace{0 \dots 0}^N|\overbrace{C \dots C}^N]^T\]

The final constraint (10) is trivial to addapt to (14) by taking 
\( A=[\overbrace{1,\dots,1}^N,\overbrace{-1,\dots,-1}^N]\) and \(b=0\).

\cleardoublepage
\chapter{Evaluation}

\cleardoublepage
\chapter{Conclusion}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

%\input{propbody}

\end{document}
